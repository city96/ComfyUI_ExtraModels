# Extra Models for ComfyUI
This repository aims to add support for various different image diffusion models to ComfyUI.
## Abstract
- ğŸ“œ[Installation](#installation)
  - ğŸ“œ[Standalone ComfyUI](#standalone-comfyui)
- ğŸ“œ[PixArt](#pixart)
  - ğŸ› [PixArt Usage](#pixart-usage)
- ğŸ“œ[PixArt Sigma](#pixart-sigma)
- ğŸ“œ[PixArt LCM](#pixart-lcm)
- ğŸ“œ[HunYuan DiT](#hunyuan-dit)
  - ğŸ› [Instructions](#instructions)
- ğŸ“œ[Dit](#dit)
  - ğŸ› [Dit Usage](#dit-usage)
- ğŸ“œ[T5](#t5)
  - ğŸ› [T5 Usage](#t5-usage)
- ğŸ“œ[MiaoBi](#miaobi)
- ğŸ“œ[VAE](#vae)
  - ğŸ“œ[Consistency Decoder](#consistency-decoder)
  - ğŸ“œ[Deflickering Decoder & VideoDecoder](#deflickering-decoder-and-videodecoder)
  - ğŸ“œ[AutoencoderKL & VQModel](#autoencoderkl-and-vqmodel)

## Installation
Simply clone this repo to your custom_nodes folder using the following command:

`git clone https://github.com/city96/ComfyUI_ExtraModels custom_nodes/ComfyUI_ExtraModels`

You will also have to install the requirements from the provided file by running `pip install -r requirements.txt` inside your VENV/conda env. If you downloaded the standalone version of ComfyUI, then follow the steps below.

### Standalone ComfyUI
I haven't tested this completely, so if you know what you're doing, use the regular venv/`git clone` install option when installing ComfyUI.

Go to the where you unpacked `ComfyUI_windows_portable` to (where your run_nvidia_gpu.bat file is) and open a command line window. Press `CTRL+SHIFT+Right click` in an empty space and click "Open PowerShell window here".

Clone the repository to your custom nodes folder, assuming haven't installed in through the manager.

`git clone https://github.com/city96/ComfyUI_ExtraModels .\ComfyUI\custom_nodes\ComfyUI_ExtraModels`

To install the requirements on windows, run these commands in the same window:
```
.\python_embeded\python.exe -s -m pip install -r .\ComfyUI\custom_nodes\ComfyUI_ExtraModels\requirements.txt
.\python_embeded\python.exe -s -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui
```

To update, open the command line window like before and run the following commands:

```
cd .\ComfyUI\custom_nodes\ComfyUI_ExtraModels\
git pull
```

Alternatively, use the manager, assuming it has an update function.

## PixArt
- [Abstract](#abstract)

[Original Repo](https://github.com/PixArt-alpha/PixArt-alpha)

### Model info / implementation
- Uses T5 text encoder instead of clip
- Available in 512 and 1024 versions, needs specific pre-defined resolutions to work correctly
- Same latent space as SD1.5 (works with the SD1.5 VAE)
- Attention needs optimization, images look worse without xformers.

### PixArt Usage
1. Download the model weights from the [PixArt alpha repo](https://huggingface.co/PixArt-alpha/PixArt-alpha/tree/main) - you most likely want the 1024px one - `PixArt-XL-2-1024-MS.pth`
2. Place them in your checkpoints folder
3. Load them with the correct PixArt checkpoint loader
4. **Follow the T5v11 section of this readme** to set up the T5 text encoder

> [!TIP]
> You should be able to use the model with the default KSampler if you're on the latest version of the node.
> In theory, this should allow you to use longer prompts as well as things like doing img2img.

Limitations:
- `PixArt DPM Sampler` requires the negative prompt to be shorter than the positive prompt.
- `PixArt DPM Sampler` can only work with a batch size of 1.
- `PixArt T5 Text Encode` is from the reference implementation, therefore it doesn't support weights. `T5 Text Encode` support weights, but I can't attest to the correctness of the implementation.

> [!IMPORTANT]  
> Installing `xformers` is optional but strongly recommended as torch SDP is only partially implemented, if that.

[Sample workflow here](https://github.com/city96/ComfyUI_ExtraModels/files/13617463/PixArtV3.json)

![](./PixArt/PixArt.jpg)

### PixArt Sigma
- [Abstract](#abstract)

The Sigma models work just like the normal ones. Out of the released checkpoints, the 512, 1024 and 2K one are supported.

You can find the [1024 checkpoint here](https://huggingface.co/PixArt-alpha/PixArt-Sigma/blob/main/PixArt-Sigma-XL-2-1024-MS.pth). Place it in your models folder and **select the appropriate type in the model loader / resolution selection node.**

> [!IMPORTANT]
> Make sure to select an SDXL VAE for PixArt Sigma!

### PixArt LCM
- [Abstract](#abstract)

The LCM model also works if you're on the latest version. To use it:

1. Download the [PixArt LCM model](https://huggingface.co/PixArt-alpha/PixArt-LCM-XL-2-1024-MS/blob/main/transformer/diffusion_pytorch_model.safetensors) and place it in your checkpoints folder.
2. Add a `ModelSamplingDiscrete` node and set "sampling" to "lcm"
3. Adjust the KSampler settings - Set the sampler to "lcm". Your CFG should be fairly low (1.1-1.5), your steps should be around 5.

Everything else can be the same the same as in the example above.

![](./PixArt/PixArtLCM.jpg)



## HunYuan DiT
- [Abstract](#abstract)

WIP implementation of [HunYuan DiT by Tencent](https://github.com/Tencent/HunyuanDiT)

> [!WARNING]
> Only a proof of concept, most things don't work yet and only 1024x1024 is supported.
> 
> The text encoder device/dtype selection also doesn't work.

The initial work on this was done by [chaojie](https://github.com/chaojie) in [this PR](https://github.com/city96/ComfyUI_ExtraModels/pull/37).

### Instructions
- Download the [first text encoder from here](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/blob/main/t2i/clip_text_encoder/pytorch_model.bin) and place it in `ComfyUI/models/clip` - rename to "chinese-roberta-wwm-ext-large.bin"
- Download the [second text encoder from here](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/blob/main/t2i/mt5/pytorch_model.bin) and place it in `ComfyUI/models/T5` - rename it to "mT5.bin"
- Download the [model file from here](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/blob/main/t2i/model/pytorch_model_module.pt) and place it in `ComfyUI/checkpoints` - rename it to "HunYuanDiT.pt"
- Download/use any SDXL VAE, for example [this one](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)

![](./HunYuanDiT/HunYuanDiT.jpg)



## DiT
- [Abstract](#abstract)

[Original Repo](https://github.com/facebookresearch/DiT)

### Model info / implementation
- Uses class labels instead of prompts
- Limited to 256x256 or 512x512 images
- Same latent space as SD1.5 (works with the SD1.5 VAE)
- Works in FP16, but no other optimization

### Dit Usage
1. Download the original model weights from the [DiT Repo](https://github.com/facebookresearch/DiT) or the converted [FP16 safetensor ones from Huggingface](https://huggingface.co/city96/DiT/tree/main).
2. Place them in your checkpoints folder. (You may need to move them if you had them in `ComfyUI\models\dit` before)
3. Load the model and select the class labels as shown in the image below
4. **Make sure to use the Empty label conditioning for the Negative input of the KSampler!**

ConditioningCombine nodes *should* work for combining multiple labels. The area ones don't since the model currently can't handle dynamic input dimensions.

[Sample workflow here](https://github.com/city96/ComfyUI_ExtraModels/files/13619259/DiTV2.json)

![](./Dit/Dit.jpg)



## T5
- [Abstract](#abstract)

### T5v11

The model files can be downloaded from the [DeepFloyd/t5-v1_1-xxl](https://huggingface.co/DeepFloyd/t5-v1_1-xxl/tree/main) repository.

You will need to download the following 4 files:
 - `config.json`
 - `pytorch_model-00001-of-00002.bin`
 - `pytorch_model-00002-of-00002.bin`
 - `pytorch_model.bin.index.json`

Place them in your `ComfyUI/models/t5` folder. You can put them in a subfolder called "t5-v1.1-xxl" though it doesn't matter. There are int8 safetensor files in the other DeepFloyd repo, thought they didn't work for me.

For faster loading/smaller file sizes, you may pick one of the following alternative downloads:
- [FP16 converted version](https://huggingface.co/theunlikely/t5-v1_1-xxl-fp16/tree/main) - Same layout as the original, download both safetensor files as well as the `*.index.json` and `config.json` files.
- [BF16 converter version](https://huggingface.co/city96/t5-v1_1-xxl-encoder-bf16/tree/main) - Merged into a single safetensor, only `model.safetensors` (+`config.json` for folder mode) are reqired.

To move T5 to a different drive/folder, do the same as you would when moving checkpoints, but add `    t5: t5` to `extra_model_paths.yaml` and create a directory called "t5" in the alternate path specified in the `base_path` variable.

### T5 Usage
Loaded onto the CPU, it'll use about 22GBs of system RAM. Depending on which weights you use, it might use slightly more during loading.

If you have a second GPU, selecting "cuda:1" as the device will allow you to use it for T5, freeing at least some VRAM/System RAM. Using FP16 as the dtype is recommended.

Loaded in bnb4bit mode, it only takes around 6GB VRAM, making it work with 12GB cards. The only drawback is that it'll constantly stay in VRAM since BitsAndBytes doesn't allow moving the weights to the system RAM temporarily. Switching to a different workflow *should* still release the VRAM as expected. Pascal cards (1080ti, P40) seem to struggle with 4bit. Select "cpu" if you encounter issues.

On windows, you may need a newer version of bitsandbytes for 4bit. Try `python -m pip install bitsandbytes`

> [!IMPORTANT]  
> You may also need to upgrade transformers and install spiece for the tokenizer. `pip install -r requirements.txt`

## MiaoBi
- [Abstract](#abstract)

### Original from: 
- Author: Github [ShineChen1024](https://github.com/ShineChen1024) | Hugging Face [ShineChen1024](https://huggingface.co/ShineChen1024)
- https://github.com/ShineChen1024/MiaoBi
- https://huggingface.co/ShineChen1024/MiaoBi
### Instructions
- Download the [clip model](https://huggingface.co/ShineChen1024/MiaoBi/blob/main/miaobi_beta0.9/text_encoder/model.safetensors) and rename it to "MiaoBi_CLIP.safetensors" or any you like, then place it in **ComfyUI/models/clip**.

è¿™æ˜¯å¦™ç¬”çš„æµ‹è¯•ç‰ˆæœ¬ã€‚å¦™ç¬”ï¼Œä¸€ä¸ªä¸­æ–‡æ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œä¸ç»å…¸çš„stable-diffusion 1.5ç‰ˆæœ¬æ‹¥æœ‰ä¸€è‡´çš„ç»“æ„ï¼Œå…¼å®¹ç°æœ‰çš„loraï¼Œcontrolnetï¼ŒT2I-Adapterç­‰ä¸»æµæ’ä»¶åŠå…¶æƒé‡ã€‚

This is the beta version of MiaoBi, a chinese text-to-image model, following the classical structure of sd-v1.5, compatible with existing mainstream plugins such as Lora, Controlnet, T2I Adapter, etc.

Example Prompts:
- ä¸€åªç²¾è‡´çš„é™¶ç“·çŒ«å’ªé›•åƒï¼Œå…¨èº«ç»˜æœ‰ç²¾ç¾çš„ä¼ ç»ŸèŠ±çº¹ï¼Œçœ¼ç›ä»¿ä½›ä¼šå‘å…‰ã€‚
- åŠ¨æ¼«é£æ ¼çš„é£æ™¯ç”»ï¼Œæœ‰å±±è„‰ã€æ¹–æ³Šï¼Œä¹Ÿæœ‰ç¹åçš„å°é•‡å­ï¼Œè‰²å½©é²œè‰³ï¼Œå…‰å½±æ•ˆæœæ˜æ˜¾ã€‚
- æå…·çœŸå®æ„Ÿçš„å¤æ‚å†œæ‘çš„è€äººè‚–åƒï¼Œé»‘ç™½ã€‚
- çº¢çƒ§ç‹®å­å¤´
- è½¦æ°´é©¬é¾™çš„ä¸Šæµ·è¡—é“ï¼Œæ˜¥èŠ‚ï¼Œèˆé¾™èˆç‹®ã€‚
- æ¯è—¤è€æ ‘æ˜é¸¦ï¼Œå°æ¡¥æµæ°´äººå®¶ã€‚æ°´å¢¨ç”»ã€‚
- **You can drag it into ComfyUI, it's a png with workflow**
![](./MiaoBi/MiaoBi.png)

**Limitations**

å¦™ç¬”çš„è®­ç»ƒæ•°æ®åŒ…å«Laion-5Bä¸­çš„ä¸­æ–‡å­é›†ï¼ˆç»è¿‡æ¸…æ´—è¿‡æ»¤ï¼‰ï¼ŒMidjourneyç›¸å…³çš„å¼€æº
æ•°æ®ï¼ˆå°†è‹±æ–‡æç¤ºè¯ç¿»è¯‘æˆä¸­æ–‡ï¼‰ï¼Œä»¥åŠæˆ‘ä»¬æ”¶é›†çš„ä¸€æ‰¹æ•°åä¸‡çš„captionæ•°æ®ã€‚ç”±äºæ•´
ä¸ªæ•°æ®é›†å¤§é‡ç¼ºå°‘æˆè¯­ä¸å¤è¯—è¯æ•°æ®ï¼Œæ‰€ä»¥å¯¹æˆè¯­ä¸å¤è¯—è¯çš„ç†è§£å¯èƒ½å­˜åœ¨åå·®ï¼Œå¯¹ä¸­
å›½çš„åèƒœåœ°æ ‡å»ºç­‘æ•°æ®çš„ç¼ºå°‘ä»¥åŠå¤§é‡çš„è‹±è¯‘ä¸­æ•°æ®ï¼Œå¯èƒ½ä¼šå¯¼è‡´å‡ºç°ä¸€äº›å¯¹è±¡çš„æ··ä¹±
ï¼Œå¦‚æœæœ‰ä»¥ä¸Šè¾ƒé«˜æ•°æ®è´¨é‡çš„ä¼™ä¼´ï¼Œå¸Œæœ›èƒ½å®Œå–„è¯¥é¡¹ç›®ï¼Œè¯·ä¸æˆ‘ä»¬è”ç³»ï¼Œæˆ‘ä»¬å°†æ ¹æ®æ
ä¾›çš„æ•°æ®è®­ç»ƒå…¨æ–°çš„ç‰ˆæœ¬ã€‚å¦™ç¬”Beta0.9åœ¨8å¼ 4090æ˜¾å¡ä¸Šå®Œæˆè®­ç»ƒï¼Œæˆ‘ä»¬æ­£åœ¨æ‹“å±•æˆ‘ä»¬
çš„æœºå™¨èµ„æºæ¥è®­ç»ƒSDXLæ¥è·å¾—æ›´ä¼˜çš„ç»“æœï¼Œæ•¬è¯·æœŸå¾…ã€‚

Due to limitations in computing power and the size of Chinese datasets, the 
performance of Miaobi may be inferior to commercial models at this stage. We
 are expanding our computing resources and collecting larger scale data, 
looking forward to the future performance of Miaobi.
## VAE
- [Abstract](#abstract)

A few custom VAE models are supported. The option to select a different dtype when loading is also possible, which can be useful for testing/comparisons. You can load the models listed below using the "ExtraVAELoader" node.

**Models like PixArt/DiT do NOT need a special VAE. Unless mentioned, use one of the following as you would with any other model:**
- [VAE for SD1.X, DiT and PixArt alpha](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors).
- [VAE for SDXL and PixArt sigma](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/blob/main/diffusion_pytorch_model.safetensors)

### Consistency Decoder
[Original Repo](https://github.com/openai/consistencydecoder)

This now works thanks to the work of @mrsteyk and @madebyollin - [Gist with more info](https://gist.github.com/madebyollin/865fa6a18d9099351ddbdfbe7299ccbf).

- Download the converted safetensor VAE from [this HF repository](https://huggingface.co/mrsteyk/consistency-decoder-sd15/blob/main/stk_consistency_decoder_amalgamated.safetensors). If you downloaded the OpenAI model before, it won't work, as it is a TorchScript file. Feel free to delete it.
- Put the file in your VAE folder
- Load it with the ExtraVAELoader
- Set it to fp16 or bf16 to not run out of VRAM
- Use tiled VAE decode if required

### Deflickering Decoder and VideoDecoder
This is the VAE that comes baked into the [Stable Video Diffusion](https://stability.ai/news/stable-video-diffusion-open-ai-video-model) model.

It doesn't seem particularly good as a normal VAE (color issues, pretty bad with finer details).

Still for completeness sake the code to run it is mostly implemented. To obtain the weights just extract them from the sdv model:

```py
from safetensors.torch import load_file, save_file

pf = "first_stage_model." # Key prefix
sd = load_file("svd_xt.safetensors")
vae = {k.replace(pf, ''):v for k,v in sd.items() if k.startswith(pf)}
save_file(vae, "svd_xt_vae.safetensors")
```

### AutoencoderKL and VQModel
`kl-f4/8/16/32` from the [compvis/latent diffusion repo](https://github.com/CompVis/latent-diffusion/tree/main#pretrained-autoencoding-models).

`vq-f4/8/16` from the taming transformers repo, weights for both vq and kl models available [here](https://ommer-lab.com/files/latent-diffusion/)

`vq-f8` can accepts latents from the SD unet but just like xl with v1 latents, output largely garbage. The rest are completely useless without a matching UNET that uses the correct channel count.

![](./vae/vae.jpg)
